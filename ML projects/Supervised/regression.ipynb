{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as skp\n",
    "from load_data import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegressionRidge(X, Y,K,lambdas,CV): \n",
    "    \n",
    "    \n",
    "    D_error=np.empty((K,len(lambdas)))\n",
    "    error=np.empty((len(lambdas)))\n",
    "    i=0\n",
    "    for train_index, test_index in CV.split(X,Y):\n",
    "        j=0\n",
    "        X_train=X[train_index]\n",
    "        Y_train=Y[train_index]\n",
    "        X_test=X[test_index]\n",
    "        Y_test=Y[test_index]\n",
    "        for l in lambdas:\n",
    "            clf = Ridge(alpha=l)\n",
    "            clf.fit(X_train,Y_train)\n",
    "            w=clf.coef_\n",
    "            Y_model=X_test @ w.T\n",
    "            D_error[i][j]=mean_squared_error(Y_model,Y_test)\n",
    "            j=j+1\n",
    "        i=i+1    \n",
    "    error=np.mean(D_error,axis=0)\n",
    "    opt_lambda=lambdas[np.argmin(error)]\n",
    "    return error, opt_lambda\n",
    "\n",
    "#definition of the function that will do the baseline\n",
    "def Baseline(X,Y,K,CV):\n",
    "    \n",
    "    split_test_error = np.empty(K)\n",
    "    j=0\n",
    "    general_error=0\n",
    "    z_base1=[]\n",
    "    for train_index, test_index in CV.split(X,Y):\n",
    "        \n",
    "        X_train=X[train_index]\n",
    "        Y_train=Y[train_index]\n",
    "        X_test=X[test_index]\n",
    "        Y_test=Y[test_index]\n",
    "        Y_predict=Y_train.mean()*np.ones(len(Y_test)) \n",
    "        z_base1=np.append(z_base1,[(Y_predict-Y_test.T)**2])\n",
    "        split_test_error[j]=mean_squared_error(Y_predict,Y_test) #calculating test error of (j+1)th split\n",
    "        general_error=general_error+split_test_error[j]*len(X_test)/len(X) #calculating the general error \n",
    "        j=j+1\n",
    "        \n",
    "               \n",
    "    return split_test_error, general_error, z_base1\n",
    "\n",
    "#function for training neural networks from toolbox_02450\n",
    "\n",
    "def train_neural_net(model, loss_fn, X, y,n_replicates=3, max_iter = 10000, tolerance=1e-6):\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Specify maximum number of iterations for training\n",
    "    logging_frequency = 1000 # display the loss every 1000th iteration\n",
    "    best_final_loss = 1e100\n",
    "    for r in range(n_replicates):\n",
    "        #print('\\n\\tReplicate: {}/{}'.format(r+1, k))\n",
    "        # Make a new net (calling model() makes a new initialization of weights) \n",
    "        net = model()\n",
    "        \n",
    "        # initialize weights based on limits that scale with number of in- and\n",
    "        # outputs to the layer, increasing the chance that we converge to \n",
    "        # a good solution\n",
    "        torch.nn.init.xavier_uniform_(net[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(net[2].weight)\n",
    "                     \n",
    "        # We can optimize the weights by means of stochastic gradient descent\n",
    "        # The learning rate, lr, can be adjusted if training doesn't perform as\n",
    "        # intended try reducing the lr. If the learning curve hasn't converged\n",
    "        # (i.e. \"flattend out\"), you can try try increasing the maximum number of\n",
    "        # iterations, but also potentially increasing the learning rate:\n",
    "        #optimizer = torch.optim.SGD(net.parameters(), lr = 2e-2)\n",
    "        \n",
    "        # A more complicated optimizer is the Adam-algortihm, which is an extension\n",
    "        # of SGD to adaptively change the learing rate, which is widely used:\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "        \n",
    "        # Train the network while displaying and storing the loss\n",
    "        #print('\\t\\t{}\\t{}\\t\\t\\t{}'.format('Iter', 'Loss','Rel. loss'))\n",
    "        learning_curve = [] # setup storage for loss at each step\n",
    "        old_loss = 1e6\n",
    "        for i in range(max_iter):\n",
    "            y_est = net(X) # forward pass, predict labels on training set\n",
    "            loss = loss_fn(y_est, y) # determine loss\n",
    "            loss_value = loss.data.numpy() #get numpy array instead of tensor\n",
    "            #learning_curve.append(loss_value) # record loss for later display\n",
    "            \n",
    "            # Convergence check, see if the percentual loss decrease is within\n",
    "            # tolerance:\n",
    "            p_delta_loss = np.abs(loss_value-old_loss)/old_loss\n",
    "            if p_delta_loss < tolerance: break\n",
    "            old_loss = loss_value\n",
    "            \n",
    "            # display loss with some frequency:\n",
    "            #if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "                #print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "                #print(print_str)\n",
    "            # do backpropagation of loss and optimize weights \n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            \n",
    "            \n",
    "        # display final loss\n",
    "        print('\\t\\tFinal loss:')\n",
    "        print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "        print(print_str)\n",
    "        \n",
    "        if loss_value < best_final_loss: \n",
    "            best_net = net\n",
    "            best_final_loss = loss_value\n",
    "            #best_learning_curve = learning_curve\n",
    "        \n",
    "    # Return the best curve along with its final loss and learing curve\n",
    "    return best_net, best_final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "data = load()\n",
    "\n",
    "#dividing inputs and output\n",
    "Y=data['CO2 Emissions'].to_numpy()\n",
    "X=data.drop('CO2 Emissions',1)\n",
    "#scaling of the inputs\n",
    "scaler = skp.StandardScaler()\n",
    "scaledX = scaler.fit_transform(X)\n",
    "\n",
    "scaledY=Y-Y.mean()\n",
    "scaledY=scaledY.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Regression: part A\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import rlr_validate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "K=5\n",
    "lambdas1=np.power(10.,range(-6,7))\n",
    "CV_A=model_selection.KFold(K,shuffle=True)\n",
    "    \n",
    "error, opt_lambda = LinearRegressionRidge(scaledX,scaledY,K, lambdas1,CV_A)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Optimal lambda: %f' %opt_lambda)\n",
    "plt.loglog(lambdas1,error,'b.-')\n",
    "plt.xlabel('Regularization factor')\n",
    "plt.ylabel(\"Squared error (crossvalidation)\")\n",
    "plt.legend(['Generalization error'])\n",
    "plt.grid()\n",
    "##the end of the part A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. iteration of the outer loop\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2107.1533\t2.8733142e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 1 is 2131.0283203125\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t372.2882\t0.00013154934\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 1 is 261.3359069824219\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t95.00132\t0.00022304669\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 1 is 22.781930923461914\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t30.26458\t0.00031589487\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 1 is 3.644876718521118\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t10.251587\t0.0005002349\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 1 is 4.350505828857422\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t3.2602255\t0.0004960835\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 1 is 6.40077018737793\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.6130146\t0.00046206004\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 1 is 6.069575786590576\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2038.4401\t3.0360357e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 2 is 2172.47705078125\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t319.83984\t0.00014920709\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 2 is 565.3934936523438\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t63.057354\t0.00021997438\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 2 is 190.0763702392578\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t21.948378\t0.00024404695\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 2 is 79.99540710449219\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t10.338258\t0.00038498337\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 2 is 42.84198760986328\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.804673\t0.00045314324\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 2 is 19.874208450317383\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.1534514\t0.00059418427\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 2 is 21.613725662231445\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2110.1704\t2.869206e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 3 is 1802.6146240234375\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t388.10287\t0.00012131554\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 3 is 282.09869384765625\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t87.01028\t0.00017551222\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 3 is 51.754085540771484\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t29.500969\t0.00027114898\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 3 is 14.410011291503906\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t11.049806\t0.0005033472\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 3 is 6.333962440490723\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t3.2835994\t0.0005353492\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 3 is 8.837543487548828\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.5845212\t0.00047164282\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 3 is 4.974173069000244\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2152.6804\t2.7785263e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 4 is 1742.057861328125\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t403.7287\t0.0001318104\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 4 is 152.28814697265625\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t95.61077\t0.00019729754\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 4 is 11.482704162597656\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t31.042591\t0.00030263784\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 4 is 3.0379087924957275\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t9.899142\t0.0005176508\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 4 is 2.943845510482788\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t3.3564842\t0.00046114164\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 4 is 4.479172706604004\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.6877073\t0.0004327982\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 4 is 3.6260032653808594\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1942.8262\t3.0032465e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 5 is 2539.3876953125\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t297.3865\t0.00014733952\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 5 is 555.998779296875\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t62.892002\t0.0002480163\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 5 is 167.94635009765625\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t20.35439\t0.00031485004\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 5 is 70.56499481201172\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t6.1343207\t0.00059142883\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 5 is 36.40719223022461\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.2353864\t0.0005360882\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 5 is 24.391752243041992\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.0120144\t0.00026496642\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 5 is 18.057931900024414\n",
      "Mean errors are [2077.51311035  363.42300415   88.80828819   34.33063974   18.57549872\n",
      "   12.79668941   10.86828194]\n",
      "Optimal model with the optimal n being:30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.7423333\t0.00036939635\n",
      "2. iteration of the outer loop\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2183.7107\t2.917916e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 1 is 2155.318115234375\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t358.05188\t0.00015015673\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 1 is 315.9691162109375\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t57.531666\t0.00025004387\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 1 is 68.28284454345703\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t19.500639\t0.000245735\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 1 is 29.09067153930664\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t7.7364454\t0.00047338047\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 1 is 18.760210037231445\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.9849732\t0.0006025177\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 1 is 16.391674041748047\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.4066148\t0.00026756595\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 1 is 18.006282806396484\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1844.0304\t3.177382e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 2 is 3241.426025390625\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t245.73047\t0.00017247192\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 2 is 875.32373046875\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t28.86162\t0.00026658588\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 2 is 245.55613708496094\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t8.787416\t0.0002504182\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 2 is 112.93624114990234\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.2736015\t0.00032547675\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 2 is 72.16633605957031\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.1594877\t0.00038494496\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 2 is 41.27370834350586\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.4021474\t0.00029382462\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 2 is 38.146156311035156\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2165.9568\t2.8854776e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 3 is 1974.538330078125\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t347.11163\t0.00015532809\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 3 is 282.3793640136719\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t64.94764\t0.0002419295\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 3 is 36.221107482910156\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t22.092278\t0.00028275524\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 3 is 3.2279932498931885\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t7.3463373\t0.00054428296\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 3 is 3.231466293334961\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.440201\t0.00051034306\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 3 is 4.282016277313232\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.4627594\t0.00023058099\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 3 is 6.027326583862305\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2335.3057\t2.7807751e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 4 is 1606.7606201171875\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t386.2426\t0.00014851941\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 4 is 128.6287841796875\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t73.19507\t0.00020925752\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 4 is 19.686647415161133\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t20.918404\t0.0002880471\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 4 is 5.313104629516602\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t6.660429\t0.00056411733\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 4 is 7.642829895019531\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.4883893\t0.0005309977\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 4 is 8.21756362915039\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.0216591\t0.00036706365\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 4 is 7.350918292999268\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2256.9602\t2.844851e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 5 is 1887.8109130859375\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t371.59174\t0.00014591773\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 5 is 202.51205444335938\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t66.82775\t0.00025395275\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 5 is 11.243505477905273\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t20.696373\t0.0002818337\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 5 is 5.472348690032959\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t7.1961684\t0.0005561642\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 5 is 7.2906718254089355\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.1785276\t0.00060090335\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 5 is 6.078526973724365\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.0565611\t0.00022030382\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 5 is 7.559263229370117\n",
      "Mean errors are [2173.17080078  360.96260986   76.1980484    31.20807185   21.81830282\n",
      "   15.24869785   15.41798944]\n",
      "Optimal model with the optimal n being:25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.84005\t0.00042560673\n",
      "3. iteration of the outer loop\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2032.3734\t2.9309853e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 1 is 1966.708251953125\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t349.11816\t0.00013486056\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 1 is 266.1086730957031\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t75.92992\t0.00024591316\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 1 is 22.774028778076172\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t26.721169\t0.00028222697\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 1 is 2.069288492202759\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t10.357848\t0.00043172625\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 1 is 2.6025798320770264\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.0107017\t0.0004534831\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 1 is 4.332398414611816\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.3497388\t0.00038805822\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 1 is 3.6272597312927246\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2138.0176\t2.8889282e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 2 is 1538.46240234375\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t373.63736\t0.00014977319\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 2 is 136.14450073242188\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t77.93291\t0.00021826256\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 2 is 7.157425880432129\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t26.805763\t0.00026462442\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 2 is 5.516049861907959\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t10.004691\t0.00044667328\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 2 is 6.200413227081299\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t3.8219552\t0.0005203635\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 2 is 6.776698589324951\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.7794083\t0.0005665133\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 2 is 7.77474308013916\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1922.4932\t3.0159614e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 3 is 2080.36962890625\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t326.52585\t0.00014409679\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 3 is 393.5088195800781\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t62.00149\t0.00021504828\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 3 is 96.80716705322266\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t22.104633\t0.00023688874\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 3 is 43.32231140136719\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t10.294223\t0.00038042731\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 3 is 24.18943977355957\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.8709626\t0.00046908285\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 3 is 12.950417518615723\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.2647939\t0.0005711933\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 3 is 17.31861686706543\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1901.5005\t3.0107409e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 4 is 2298.7314453125\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t323.34448\t0.000119755154\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 4 is 440.0639953613281\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t65.05707\t0.00020483282\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 4 is 81.61882019042969\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t26.463976\t0.00022424212\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 4 is 30.006582260131836\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t11.492436\t0.00042435975\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 4 is 12.886408805847168\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.6806145\t0.0005093187\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 4 is 14.26786994934082\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.202628\t0.00045960402\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 4 is 7.100834369659424\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1950.1151\t3.0483548e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 5 is 2097.951171875\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t293.01227\t0.00015922182\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 5 is 478.7079162597656\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t47.64862\t0.00023867872\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 5 is 170.86279296875\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t15000\t15.796579\t0.00028958195\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 5 is 90.21087646484375\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t5.7562814\t0.00047435213\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 5 is 51.148155212402344\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.4913318\t0.0004796059\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 5 is 38.9844856262207\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.5033382\t0.00029727304\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 5 is 35.37062454223633\n",
      "Mean errors are [1996.44458008  342.90678101   75.84404697   34.2250217    19.40539937\n",
      "   15.46237402   14.23841572]\n",
      "Optimal model with the optimal n being:30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.3271441\t0.00039274583\n",
      "4. iteration of the outer loop\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2142.7332\t2.8711775e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 1 is 1434.8760986328125\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t392.97964\t0.0001380549\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 1 is 179.1887969970703\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t84.60689\t0.00019473926\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 1 is 21.026378631591797\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t29.502817\t0.0002701628\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 1 is 3.83980655670166\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t10.78247\t0.00044150802\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 1 is 2.708608865737915\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.414373\t0.0005117491\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 1 is 5.632055759429932\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.0569563\t0.000588121\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 1 is 3.7063653469085693\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2045.085\t2.8829249e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 2 is 2226.033203125\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t349.6035\t0.00012978632\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 2 is 323.1100158691406\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t80.89993\t0.0001644436\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 2 is 38.283687591552734\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t28.954903\t0.0002721135\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 2 is 11.493067741394043\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t11.235197\t0.00042813492\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 2 is 13.97169017791748\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.297188\t0.0005080721\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 2 is 10.130142211914062\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.0682213\t0.0003792331\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 2 is 14.445842742919922\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1978.0347\t2.9868166e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 3 is 2420.23876953125\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t300.5295\t0.00014640794\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 3 is 578.7340698242188\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t38.199062\t0.00027305246\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 3 is 216.42807006835938\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t13.287643\t0.00021390512\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 3 is 112.54243469238281\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t6.9648066\t0.00033426547\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 3 is 69.98397064208984\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t3.1644027\t0.000521861\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 3 is 43.45794677734375\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.3632865\t0.0006094529\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 3 is 33.39341735839844\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1996.841\t2.909785e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 4 is 2019.43115234375\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t360.56436\t0.00013557228\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 4 is 276.4336242675781\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t77.57665\t0.00020992577\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 4 is 19.742563247680664\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t28.460236\t0.0002579528\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 4 is 5.81849479675293\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t11.367462\t0.00042332316\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 4 is 5.509087085723877\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.1271496\t0.00052091444\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 4 is 10.540140151977539\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.0352983\t0.00039566564\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 4 is 7.743328094482422\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1974.3707\t2.9799945e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 5 is 2102.738037109375\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t324.8621\t0.00014361378\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 5 is 406.91064453125\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t59.85825\t0.00023427607\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 5 is 99.57955932617188\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t21.612787\t0.00023663299\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 5 is 48.49591827392578\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t10.39943\t0.0003591691\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 5 is 27.74840545654297\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t5.2440324\t0.0004055628\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 5 is 21.761932373046875\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.585304\t0.0005125765\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 5 is 17.47736358642578\n",
      "Mean errors are [2040.66345215  352.8754303    79.01205177   36.43794441   23.98435245\n",
      "   18.30444345   15.35326343]\n",
      "Optimal model with the optimal n being:30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.2032223\t0.00030290626\n",
      "5. iteration of the outer loop\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2078.293\t2.901467e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 1 is 2074.938720703125\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t325.91293\t0.00014839295\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 1 is 384.0079650878906\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t69.697495\t0.000200608\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 1 is 90.72701263427734\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t27.13169\t0.00022989734\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 1 is 32.2244987487793\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t11.914674\t0.0004283613\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 1 is 11.372220993041992\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.4615464\t0.00054616417\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 1 is 6.892065048217773\n",
      "\n",
      "\tReplicate: 1/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.1034386\t0.0003706208\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 1 is 9.617073059082031\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2098.2917\t2.8389104e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 2 is 2014.1695556640625\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t380.55255\t0.00013807295\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 2 is 250.91201782226562\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t85.06497\t0.0002057043\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 2 is 16.71542739868164\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t30.345806\t0.0002717059\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 2 is 5.085234642028809\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t10.751732\t0.00051631976\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 2 is 4.66163444519043\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t3.5854366\t0.0005261085\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 2 is 4.6048760414123535\n",
      "\n",
      "\tReplicate: 2/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.6663756\t0.00043612038\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 2 is 4.238624572753906\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2100.7285\t2.9169647e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 3 is 2248.20361328125\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t336.06335\t0.0001410065\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 3 is 363.1485595703125\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t79.672356\t0.00019483286\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 3 is 44.8779182434082\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t31.763601\t0.00027254474\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 3 is 5.568660736083984\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t11.372594\t0.0005054874\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 3 is 5.619446277618408\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t3.8815498\t0.0004925584\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 3 is 8.590116500854492\n",
      "\n",
      "\tReplicate: 3/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.8797953\t0.00039011045\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 3 is 6.823861598968506\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2069.99\t2.9248984e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 4 is 2073.528076171875\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t337.84543\t0.00014595198\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 4 is 371.0079650878906\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t69.397575\t0.00021510138\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 4 is 95.86396026611328\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t24.924168\t0.00024145796\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 4 is 35.44954299926758\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t11.42705\t0.00042603645\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 4 is 15.200678825378418\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t4.4567738\t0.0005219532\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 4 is 5.104085922241211\n",
      "\n",
      "\tReplicate: 4/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.4174998\t0.0004361133\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 4 is 4.239222049713135\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=1\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2041.5189\t2.9118766e-05\n",
      "Individual error in inner loop for h= 1 and for iteration K2= 5 is 2037.6751708984375\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=5\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t347.1238\t0.00015154343\n",
      "Individual error in inner loop for h= 5 and for iteration K2= 5 is 390.7257080078125\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=10\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t58.353336\t0.00023861766\n",
      "Individual error in inner loop for h= 10 and for iteration K2= 5 is 135.6548614501953\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=15\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t18.989954\t0.00026228014\n",
      "Individual error in inner loop for h= 15 and for iteration K2= 5 is 72.93913269042969\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=20\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t7.622555\t0.00047657522\n",
      "Individual error in inner loop for h= 20 and for iteration K2= 5 is 39.82023239135742\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=25\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.8762822\t0.0005337831\n",
      "Individual error in inner loop for h= 25 and for iteration K2= 5 is 22.099365234375\n",
      "\n",
      "\tReplicate: 5/5\n",
      "\n",
      "\t h=30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t1.456525\t0.00014648109\n",
      "Individual error in inner loop for h= 30 and for iteration K2= 5 is 15.426112174987793\n",
      "Mean errors are [2089.70302734  351.96044312   76.767836     30.25341396   15.33484259\n",
      "    9.45810175    8.06897869]\n",
      "Optimal model with the optimal n being:30\n",
      "\t\tFinal loss:\n",
      "\t\t15000\t2.1929214\t0.00030574118\n",
      "Baseline test error is [3552.09338818 3502.59985128 2890.83758815 3095.18216762 3263.39672347]\n",
      "Ridge test errors are [14.71717274  5.52473799  8.86684624  6.88292849 21.23610443] and values for optimal lambda are: [0.1 0.1 1.  1.  1. ]\n",
      "Individual test errors are: [ 5.65957642 19.67116356  6.06267643  4.11362791 12.04763603]  and optimal values of h are: [30, 25, 30, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "n_hidden_units = [1,5,10,15,20,25,30]    # number of hidden units\n",
    "n_replicates = 1        # number of networks trained in each k-fold\n",
    "max_iter = 15000        # \n",
    "K1=5\n",
    "K2=5\n",
    "CVo = model_selection.KFold(K1, shuffle=True)\n",
    "CVi = model_selection.KFold(K2,shuffle=True)\n",
    "errors_inner=np.empty((K2,len(n_hidden_units)))\n",
    "errors_mean=np.empty(len(n_hidden_units))\n",
    "test_errors=np.empty(K1)\n",
    "\n",
    "opt_n=[0]*K1\n",
    "\n",
    "#training of the baseline model\n",
    "baseline_test_error, baseline_generalization_error,z_base=Baseline(scaledX,scaledY,K1,CVo)\n",
    "\n",
    "\n",
    "#training of the linear regression model\n",
    "\n",
    "i=0\n",
    "opt_lambda_array=np.empty(K1) #optimal lambda for each of the CV splits\n",
    "lambd=np.power(10.,range(-6,7))\n",
    "ridge_test_error=np.empty(K1)\n",
    "z_regr=[]\n",
    "z_ANN=[]\n",
    "for train_index, test_index in CVo.split(scaledX,scaledY):\n",
    "    \n",
    "    X_test=scaledX[test_index]\n",
    "    X_par=scaledX[train_index]\n",
    "    Y_test=scaledY[test_index]\n",
    "    Y_par=scaledY[train_index]\n",
    "    error_val, opt_lambda = LinearRegressionRidge(X_par,Y_par,K2, lambd,CVi)\n",
    "    opt_lambda_array[i]=opt_lambda\n",
    "    \n",
    "    \n",
    "    clf = Ridge(alpha=opt_lambda)\n",
    "    clf.fit(X_par,Y_par)\n",
    "    w=clf.coef_\n",
    "    Y_model=X_test @ w.T\n",
    "    z_regr=np.append(z_regr,[(Y_model-Y_test)**2])\n",
    "    ridge_test_error[i]=mean_squared_error(Y_model,Y_test)\n",
    "    i=i+1    \n",
    "ridge_general_error=ridge_test_error.mean()\n",
    "\n",
    "\n",
    "#training of the ANN\n",
    "m=0\n",
    "for train_index, test_index in CVo.split(scaledX,scaledY): #outer loop\n",
    "    print('{}. iteration of the outer loop'.format(m+1))\n",
    "    i=0\n",
    "    j=0\n",
    "    X_test=scaledX[test_index]\n",
    "    X_par=scaledX[train_index]\n",
    "    Y_test=scaledY[test_index]\n",
    "    Y_par=scaledY[train_index]\n",
    "    for train_index, test_index in CVi.split(X_par,Y_par): #inner loop\n",
    "        l=0\n",
    "        X_train=torch.tensor(X_par[train_index],dtype=torch.float) #converting to pytorch tensors\n",
    "        Y_train=torch.tensor(Y_par[train_index],dtype=torch.float)\n",
    "        X_val=torch.tensor(X_par[test_index],dtype=torch.float)\n",
    "        Y_val=torch.tensor(Y_par[test_index],dtype=torch.float)\n",
    "        for n in n_hidden_units: #creating neural nerwork\n",
    "            \n",
    "            model = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(X_par.shape[1], n), #M features to n_hidden_units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(n, 1), # n_hidden_units to 1 output neuron\n",
    "                    # no final tranfer function, i.e. \"linear output\"\n",
    "                    )\n",
    "            loss_fn = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "            print('\\n\\tReplicate: {}/{}'.format(i+1, K2))\n",
    "            print('\\n\\t h={}'.format(n))\n",
    "\n",
    "            net, final_loss= train_neural_net(model,\n",
    "                                                       loss_fn,\n",
    "                                                       X=X_train,\n",
    "                                                       y=Y_train,\n",
    "                                                       n_replicates=1,\n",
    "                                                       max_iter=max_iter)\n",
    "            y_val_est = net(X_val)\n",
    "    \n",
    "            # Determine errors and errors\n",
    "            se = (y_val_est.float()-Y_val.float())**2 # squared error\n",
    "            errors_inner[j][l] = (sum(se).type(torch.float)/len(Y_val)).data.numpy() #mean\n",
    "            print('Individual error in inner loop for h=',n,'and for iteration K2=',j+1,'is',errors_inner[j][l])\n",
    "            l=l+1\n",
    "        i=i+1\n",
    "        j=j+1\n",
    "   \n",
    "    errors_mean=np.mean(errors_inner,axis=0)\n",
    "    opt_n[m]=n_hidden_units[np.argmin(errors_mean)]\n",
    "    print('Mean errors are',errors_mean)\n",
    "    XtestTensor=torch.tensor(X_test,dtype=torch.float)\n",
    "    XparTensor=torch.tensor(X_par,dtype=torch.float)\n",
    "    YtestTensor=torch.tensor(Y_test,dtype=torch.float)\n",
    "    YparTensor=torch.tensor(Y_par,dtype=torch.float)\n",
    "    #training the optimal model\n",
    "    model1 = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(X_par.shape[1], opt_n[m]), #M features to n_hidden_units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(opt_n[m], 1), # n_hidden_units to 1 output neuron\n",
    "                    # no final tranfer function, i.e. \"linear output\"\n",
    "                    )\n",
    "    loss_fn1 = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "    print('Optimal model with the optimal n being:{}'.format(opt_n[m]))\n",
    "    \n",
    "\n",
    "    net1, final_loss= train_neural_net(model1,loss_fn1,X=XparTensor, y=YparTensor, n_replicates=1, max_iter=max_iter)\n",
    "    y_val_est1 = net1(XtestTensor)\n",
    "    square_error_outer = (y_val_est1.float()-YtestTensor.float())**2 # squared error\n",
    "    z_ANN=np.append(z_ANN,[square_error_outer.data.numpy()])\n",
    "    test_errors[m] = (sum(square_error_outer).type(torch.float)/len(YtestTensor)).data.numpy() #mean\n",
    "    m=m+1\n",
    "\n",
    "print('Baseline test error is',baseline_test_error)    \n",
    "print('Ridge test errors are',ridge_test_error,'and values for optimal lambda are:',opt_lambda_array)\n",
    "print('Individual test errors are:',test_errors,' and optimal values of h are:',opt_n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence interval is (-3581.496357549624, -2918.3047477487) and p-value is 6.961208047747079e-71\n",
      "Confidence interval is (-6.716466144875399, 10.568174968954674) and p-value is 0.3310015915771086\n",
      "Confidence interval is (-3583.4720792981097, -2920.1807348242937) and p-value is 6.171694947256586e-71\n"
     ]
    }
   ],
   "source": [
    "#t-test, confidence interval and p-value\n",
    "import scipy.stats as st\n",
    "\n",
    "#1.Linear regression vs Baseline\n",
    "alpha=0.05\n",
    "z1 = z_regr - z_base\n",
    "CI1 = st.t.interval(1-alpha, len(z1)-1, loc=np.mean(z1), scale=st.sem(z1))  # Confidence interval\n",
    "p1 = st.t.cdf( -np.abs( np.mean(z1) )/st.sem(z1), df=len(z1)-1)  # p-value\n",
    "print('Confidence interval is',CI1,'and p-value is',p1)\n",
    "\n",
    "#2.Linear regression vs ANN\n",
    "z2 = z_regr - z_ANN\n",
    "CI2 = st.t.interval(1-alpha, len(z2)-1, loc=np.mean(z2), scale=st.sem(z2))  # Confidence interval\n",
    "p2 = st.t.cdf( -np.abs( np.mean(z2) )/st.sem(z2), df=len(z2)-1)  # p-value\n",
    "print('Confidence interval is',CI2,'and p-value is',p2)\n",
    "\n",
    "#3. ANN vs baseline\n",
    "z3 = z_ANN - z_base\n",
    "CI3 = st.t.interval(1-alpha, len(z3)-1, loc=np.mean(z3), scale=st.sem(z3))  # Confidence interval\n",
    "p3 = st.t.cdf( -np.abs( np.mean(z3) )/st.sem(z3), df=len(z3)-1)  # p-value\n",
    "print('Confidence interval is',CI3,'and p-value is',p3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
